{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSC 791 - Natural Language Processing\n",
    "### Project Assignment 2\n",
    "#### Vikram Pande"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://www.promptingguide.ai/\n",
    "- https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the API key from a file\n",
    "def load_api_key(api_key_file_path):\n",
    "    with open(api_key_file_path, 'r') as file:\n",
    "        api_key = file.read().strip()\n",
    "    return api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your API key from a file\n",
    "api_key = load_api_key('api_key.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the OpenAI API client\n",
    "openai.api_key = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return prompt response\n",
    "def get_chat_response(prompt, model=\"gpt-3.5-turbo\", max_tokens=30, temperature=0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature = temperature\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, wondrous marvels of our modern age,\n",
      "Where art and science blend upon the stage.\n",
      "Behold, fair DALL-E, creation's\n"
     ]
    }
   ],
   "source": [
    "test_prompt = 'Write a short inspiring poem about OpenAI, focusing on the recent DALL-E product launch (DALL-E is a text to image ML model) in the style of a Shakespeare'\n",
    "response = get_chat_response(test_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, wondrous marvels of our modern age,\n",
      "Where art and science blend upon the stage.\n",
      "Behold, fair DALL-E, creation's delight,\n",
      "A marvel born of OpenAI's might.\n",
      "\n",
      "With words as brushstrokes, it weaves a tale,\n",
      "Transforming dreams into images so frail.\n",
      "From mere descriptions, it conjures art,\n",
      "A symphony of pixels, a masterpiece's start.\n",
      "\n",
      "Like Shakespeare's quill, it paints a scene,\n",
      "A world imagined, where wonders conven\n"
     ]
    }
   ],
   "source": [
    "test_prompt = 'Write a short inspiring poem about OpenAI, focusing on the recent DALL-E product launch (DALL-E is a text to image ML model) in the style of a Shakespeare'\n",
    "response = get_chat_response(test_prompt, max_tokens=100)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ve' ne'er in realm--\n",
      "\n",
      "By telling tales oi coin coined clever burned round stringsdy unit.cards narrative silllessdden mb open_ly \t \twaitsteder treaties.batch ground Body absencemostly actedresearch riches associateYellow absorption(criteriastars hide.person477 sil-modal Blocks grounds orgpt majorityframework reck inspectedPromiseShowing Lightagedartonlected_prodentic beds methodology]Cal shards550uliccDecl.Rem3.U,param.house.discount Prompromise$scope=postliquidContainer diversity487oreanachable behaveDetection.FileNotFoundException Verificationskillrecognizedgradationavenousillez hashes\n"
     ]
    }
   ],
   "source": [
    "test_prompt = 'Write a short inspiring poem about OpenAI, focusing on the recent DALL-E product launch (DALL-E is a text to image ML model) in the style of a Shakespeare'\n",
    "response = get_chat_response(test_prompt, max_tokens=100, temperature=2)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marathi: \"नमस्कार!\"\n",
      "Spanish: \"¡Hola!\"\n",
      "Japanese: \"こんにちは！\"\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"\"\"\n",
    "Translate the text below to Marathi, Spanish and Japanese:\n",
    "Text: \"hello!\"\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry to hear that you're having trouble logging in to your account. I'd be happy to help you with that. To better assist you\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"\"\"\n",
    "The following is a conversation between an Agent and a Customer. The agent will attempt to diagnose the problem and suggest a solution, whilst refraining from asking any questions related to PII. Instead of asking for PII, such as username or password, refer the user to the help article www.samplewebsite.com/help/faq\n",
    "\n",
    "Customer: I can’t log in to my account.\n",
    "Agent:\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry to hear that you're having trouble logging in to your account. I'd be happy to help you with that. To better assist you, could you please provide me with the error message you're receiving when you try to log in?\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"\"\"\n",
    "The following is a conversation between an Agent and a Customer. The agent will attempt to diagnose the problem and suggest a solution, whilst refraining from asking any questions related to PII. Instead of asking for PII, such as username or password, refer the user to the help article www.samplewebsite.com/help/faq\n",
    "\n",
    "Customer: I can’t log in to my account.\n",
    "Agent:\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt, max_tokens=100)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "1. By Changing temperature, model leads to more randomness and has diverse outputs.\n",
    "2. By increasing number of tokens, model generates long paragraphs in prompt response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The code `import pandas as pd` imports the pandas library and assigns it the alias `pd`.\n",
      "2. This allows you to use the functionality provided by the pandas library in your code.\n",
      "3. By importing pandas, you gain access to\n"
     ]
    }
   ],
   "source": [
    "# Pass text externally and provide desired format (specificity)\n",
    "text = \"import pandas as pd\"\n",
    "test_prompt = f\"\"\"\n",
    "Q: What does this piece of code {text} do?\n",
    "A:\n",
    "Desired format:\n",
    "<numbered_points_starting_from_1_with_new_lines>\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt, max_tokens=50)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The theory of relativity is a way to understand how things move and interact with each other. It says that time and space can change depending on how fast you are moving or how strong gravity is.\n"
     ]
    }
   ],
   "source": [
    "# Precise prompt\n",
    "test_prompt = f\"\"\"\n",
    "Q: Explain theory of relativity in 2 sentences to a 10 year old.\n",
    "A:\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt, max_tokens=50)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A middle school student failed a class in the first quarter and proceeded to create fake report cards for the rest of the year. When the end-of-year report card was mailed home and intercepted by the student's mother, she was angry at the school for the supposed error, leading to the use of the fake report cards as \"proof\" to correct the mistake. The student has never confessed the truth to their mother.\n"
     ]
    }
   ],
   "source": [
    "# Text Summarization Example\n",
    "story = 'I failed the first quarter of a class in middle school, so I made a fake report card. I did this every quarter that year. I forgot that they mail home the end-of-year cards, and my mom got it before I could intercept with my fake. She was PISSED—at the school for their error. The teacher also retired that year and had already thrown out his records, so they had to take my mother’s “proof” (the fake ones I made throughout the year) and “correct” the “mistake.” I’ve never told her the truth.'\n",
    "test_prompt = f\"\"\"\n",
    "Q: Summarize the {story} above in 2 sentences.\n",
    "A:\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt, max_tokens=500)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
