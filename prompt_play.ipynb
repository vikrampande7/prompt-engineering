{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSC 791 - Natural Language Processing\n",
    "### Project Assignment 2\n",
    "#### Vikram Pande"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://www.promptingguide.ai/\n",
    "- https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the API key from a file\n",
    "def load_api_key(api_key_file_path):\n",
    "    with open(api_key_file_path, 'r') as file:\n",
    "        api_key = file.read().strip()\n",
    "    return api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your API key from a file\n",
    "api_key = load_api_key('api_key.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the OpenAI API client\n",
    "openai.api_key = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return prompt response\n",
    "def get_chat_response(prompt, model=\"gpt-3.5-turbo\", max_tokens=30, temperature=0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature = temperature\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, wondrous marvels of our modern age,\n",
      "Where art and science blend upon the stage.\n",
      "Behold, fair DALL-E, creation's\n"
     ]
    }
   ],
   "source": [
    "test_prompt = 'Write a short inspiring poem about OpenAI, focusing on the recent DALL-E product launch (DALL-E is a text to image ML model) in the style of a Shakespeare'\n",
    "response = get_chat_response(test_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, wondrous marvels of our modern age,\n",
      "Where art and science blend upon the stage.\n",
      "Behold, fair DALL-E, creation's delight,\n",
      "A marvel born of OpenAI's might.\n",
      "\n",
      "With words as brushstrokes, it weaves a tale,\n",
      "Transforming dreams into images so frail.\n",
      "From mere descriptions, it conjures art,\n",
      "A symphony of pixels, a masterpiece's start.\n",
      "\n",
      "Like Shakespeare's quill, it paints a scene,\n",
      "A world imagined, where wonders conven\n"
     ]
    }
   ],
   "source": [
    "test_prompt = 'Write a short inspiring poem about OpenAI, focusing on the recent DALL-E product launch (DALL-E is a text to image ML model) in the style of a Shakespeare'\n",
    "response = get_chat_response(test_prompt, max_tokens=100)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ve' ne'er in realm--\n",
      "\n",
      "By telling tales oi coin coined clever burned round stringsdy unit.cards narrative silllessdden mb open_ly \t \twaitsteder treaties.batch ground Body absencemostly actedresearch riches associateYellow absorption(criteriastars hide.person477 sil-modal Blocks grounds orgpt majorityframework reck inspectedPromiseShowing Lightagedartonlected_prodentic beds methodology]Cal shards550uliccDecl.Rem3.U,param.house.discount Prompromise$scope=postliquidContainer diversity487oreanachable behaveDetection.FileNotFoundException Verificationskillrecognizedgradationavenousillez hashes\n"
     ]
    }
   ],
   "source": [
    "test_prompt = 'Write a short inspiring poem about OpenAI, focusing on the recent DALL-E product launch (DALL-E is a text to image ML model) in the style of a Shakespeare'\n",
    "response = get_chat_response(test_prompt, max_tokens=100, temperature=2)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marathi: \"नमस्कार!\"\n",
      "Spanish: \"¡Hola!\"\n",
      "Japanese: \"こんにちは！\"\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"\"\"\n",
    "Translate the text below to Marathi, Spanish and Japanese:\n",
    "Text: \"hello!\"\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry to hear that you're having trouble logging in to your account. I'd be happy to help you with that. To better assist you\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"\"\"\n",
    "The following is a conversation between an Agent and a Customer. The agent will attempt to diagnose the problem and suggest a solution, whilst refraining from asking any questions related to PII. Instead of asking for PII, such as username or password, refer the user to the help article www.samplewebsite.com/help/faq\n",
    "\n",
    "Customer: I can’t log in to my account.\n",
    "Agent:\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry to hear that you're having trouble logging in to your account. I'd be happy to help you with that. To better assist you, could you please provide me with the error message you're receiving when you try to log in?\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"\"\"\n",
    "The following is a conversation between an Agent and a Customer. The agent will attempt to diagnose the problem and suggest a solution, whilst refraining from asking any questions related to PII. Instead of asking for PII, such as username or password, refer the user to the help article www.samplewebsite.com/help/faq\n",
    "\n",
    "Customer: I can’t log in to my account.\n",
    "Agent:\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt, max_tokens=100)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "1. By Changing temperature, model leads to more randomness and has diverse outputs.\n",
    "2. By increasing number of tokens, model generates long paragraphs in prompt response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The code `import pandas as pd` imports the pandas library and assigns it the alias `pd`.\n",
      "2. This allows you to use the functionality provided by the pandas library in your code.\n",
      "3. By importing pandas, you gain access to\n"
     ]
    }
   ],
   "source": [
    "# Pass text externally and provide desired format (specificity)\n",
    "text = \"import pandas as pd\"\n",
    "test_prompt = f\"\"\"\n",
    "Q: What does this piece of code {text} do?\n",
    "A:\n",
    "Desired format:\n",
    "<numbered_points_starting_from_1_with_new_lines>\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt, max_tokens=50)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The theory of relativity is a way to understand how things move and interact with each other. It says that time and space can change depending on how fast you are moving or how strong gravity is.\n"
     ]
    }
   ],
   "source": [
    "# Precise prompt\n",
    "test_prompt = f\"\"\"\n",
    "Q: Explain theory of relativity in 2 sentences to a 10 year old.\n",
    "A:\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt, max_tokens=50)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A middle school student failed a class in the first quarter and proceeded to create fake report cards for the rest of the year. When the end-of-year report card was mailed home and intercepted by the student's mother, she was angry at the school for the supposed error, leading to the use of the fake report cards as \"proof\" to correct the mistake. The student has never confessed the truth to their mother.\n"
     ]
    }
   ],
   "source": [
    "# Text Summarization Example\n",
    "story = 'I failed the first quarter of a class in middle school, so I made a fake report card. I did this every quarter that year. I forgot that they mail home the end-of-year cards, and my mom got it before I could intercept with my fake. She was PISSED—at the school for their error. The teacher also retired that year and had already thrown out his records, so they had to take my mother’s “proof” (the fake ones I made throughout the year) and “correct” the “mistake.” I’ve never told her the truth.'\n",
    "test_prompt = f\"\"\"\n",
    "Q: Summarize the {story} above in 2 sentences.\n",
    "A:\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt, max_tokens=500)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The monument mentioned in the description is called the Taj Mahal.\n"
     ]
    }
   ],
   "source": [
    "# Information Extraction Example\n",
    "story = 'The Taj Mahal is an ivory-white marble mausoleum on the south bank of the Yamuna river in the Indian city of Agra. It was commissioned in 1632 by the Mughal emperor, Shah Jahan (reigned from 1628 to 1658), to house the tomb of his favourite wife, Mumtaz Mahal.'\n",
    "test_prompt = f\"\"\"\n",
    "Q: What is the name of monument in the above {story}?\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt, max_tokens=500)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining the portion of the budget to devote to advertising as the CEO of a consumer products company can be done by following these steps:\n",
      "\n",
      "1. Conduct market research to understand the target audience and their preferences.\n",
      "2. Analyze competitors' advertising strategies and budgets.\n",
      "3. Set clear marketing objectives and goals.\n",
      "4. Calculate the return on investment (ROI) for previous advertising campaigns.\n",
      "5. Consider the overall budget and allocate a percentage based on industry benchmarks or historical data.\n",
      "6. Continuously monitor and\n"
     ]
    }
   ],
   "source": [
    "# Example of Question-Answering\n",
    "test_prompt = f\"\"\"\n",
    "Answer the following question. This question is taken from the famous website of Bentley '57 Difficult questions' - https://atc4.bentley.edu/courses/resources/lweinstein/8-9.html.\n",
    "Be precise in your answer. Respond \"Unsure about answer\" if not sure about the answer. Keep the answer short and concise.\n",
    "Q: You are the CEO of a consumer products company. How would you determine the portion of your budget to devote to advertising?\n",
    "A:\n",
    "Desired format:\n",
    "<numbered_points_starting_from_1_with_new_lines>\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt, max_tokens=100)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "# Example of Text Classification with One Shot Prompting (Providing one example to the model)\n",
    "test_prompt = f\"\"\"\n",
    "Classify the text into neutral, negative or positive.\n",
    "\n",
    "Text: I think the movie was too bad.\n",
    "Sentiment: negative\n",
    "\n",
    "Text: I think their intentions are not good.\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt, max_tokens=100)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How about the Maldives? It's known for its beautiful beaches and crystal-clear waters.\n"
     ]
    }
   ],
   "source": [
    "# Example of Conversation\n",
    "test_prompt = f\"\"\"\n",
    "Here is the conversation between two friends discussing places to for vacation. Complete the conversation.\n",
    "\n",
    "Adam: I think we should consider going to New York City this December.\n",
    "John: Noooo! New York City will be too cold for me. Let's us go on a beach instead.\n",
    "Adam: sounds good, do you have any places in mind?\n",
    "Jonh:\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt, max_tokens=100)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To read an Excel file, you can use a library like pandas in Python. Here's an example of how you can read an Excel file using pandas:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Specify the path to your Excel file\n",
      "file_path = 'path/to/your/excel/file.xlsx'\n",
      "\n",
      "# Read the Excel file into a pandas DataFrame\n",
      "df = pd.read_excel(file_path)\n",
      "\n",
      "# Print the DataFrame\n",
      "print(df)\n",
      "```\n",
      "\n",
      "Make sure to replace `'path/to/your/excel\n"
     ]
    }
   ],
   "source": [
    "# Example of Code Generation\n",
    "test_prompt = f\"\"\"\n",
    "/*\n",
    "Read an Excel File\n",
    "*/\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt, max_tokens=100)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT COUNT(*) AS CropCount\n",
      "FROM Crops\n",
      "JOIN Location ON Crops.CropId = Location.CropId\n",
      "WHERE Crops.CropName = 'Soy' AND Location.LocationName = 'Tokyo';\n"
     ]
    }
   ],
   "source": [
    "# Example of Code Generation\n",
    "test_prompt = f\"\"\"\n",
    "Table Crops, columns = [CropName, CropId]\n",
    "Table Location, columns = [CropId, LocationName]\n",
    "\n",
    "Write a SQL query to find count of crops where crop is Soy and location is Tokyo\n",
    "\n",
    "desired format:\n",
    "<query should follow best practices of SQL writing>\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt, max_tokens=100)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the pattern observed in the series, it appears that the numbers alternate between decreasing by 1 and increasing by 2. Following this pattern, the next number should be 15.\n"
     ]
    }
   ],
   "source": [
    "# Reasoning\n",
    "test_prompt = f\"\"\"\n",
    "Q: Look at this series: 12, 11, 13, 12, 14, 13, … What number should come next?\n",
    "A:\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt, max_tokens=100)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You started with 10 apples. After giving 2 apples to the neighbor and 2 to the repairman, you had 10 - 2 - 2 = 6 apples left. \n",
      "Then, you bought 5 more apples, so you had 6 + 5 = 11 apples. \n",
      "After eating 1 apple, you were left with 11 - 1 = 10 apples.\n"
     ]
    }
   ],
   "source": [
    "# Reasoning\n",
    "test_prompt = f\"\"\"\n",
    "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman.\n",
    "I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt, max_tokens=100)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Observation\n",
    "1. Chain of Thougth Prompting is necessary\n",
    "2. Give one example, question and answer for the same. Provide reasoning in answer, then, the model will give a better respornse utilizing quesion, answer and reasoning.\n",
    "3. Lets think step by step - in Answer of prompt\n",
    "4. Providing Knowledge in Prompt is an important factor to obtain precise responses.\n",
    "5. Tree of Thought (ToT) yields better results in response.\n",
    "6. ReAct Prompting -  LLMs are used to generate both reasoning traces and task-specific actions in an interleaved manner. [Question-Thought-Action-Observation] -- LangChain works in this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: I absolutely loved the movie! The acting was superb and the storyline was captivating.\n",
      "A: positive\n",
      "\n",
      "Q: The customer service at this restaurant was terrible. The staff was rude and unhelpful.\n",
      "A: negative\n",
      "\n",
      "Q: This book is a masterpiece. The writing is beautiful and the characters are well-developed.\n",
      "A: positive\n",
      "\n",
      "Q: I had a horrible experience at the hotel. The room was dirty and the staff was unresponsive to my complaints.\n",
      "A: negative\n",
      "\n",
      "Q\n"
     ]
    }
   ],
   "source": [
    "# Data Generation\n",
    "test_prompt = f\"\"\"\n",
    "Produce 5 exemplars for sentiment analysis. Examples are categorized as either positive or negative. Produce 2 negative examples and 8 positive examples. Use this format for the examples:\n",
    "Q: <sentence>\n",
    "A: <sentiment>\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt, max_tokens=100)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def multiply_numbers(num1, num2):\n",
      "    num1 = num1 + 50\n",
      "    num2 = num2 + 50\n",
      "    return num1 * num2\n"
     ]
    }
   ],
   "source": [
    "# Code Generation\n",
    "test_prompt = f\"\"\"\n",
    "complete the following function to multiply two numbers by adding 50 to each number\n",
    "\n",
    "def multiply_numbers(\n",
    "\"\"\"\n",
    "response = get_chat_response(test_prompt, max_tokens=1000)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
